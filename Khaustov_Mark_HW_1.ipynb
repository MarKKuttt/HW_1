{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import WordPunctTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the file txt\n",
    "with open('/Users/markkhaus/Documents/GitHub/hse/Armageddon2419-A.D..txt', 'r') as f:\n",
    "    data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project\\tGutenberg's\\tArmageddon--2419\\tA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,\\tby\\tPhilip\\tFrancis\\tNowlan This\\teBook\\tis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\t\\tYou\\tmay\\tcopy\\tit,\\tgive\\tit\\taway\\tor\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>org\\n\\nTitle:\\tArmageddon--2419\\tA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nAuthor:\\tPhilip\\tFrancis\\tNowlan\\n\\nIllust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\tPaul\\n\\nRelease\\tDate:\\tMay\\t26,\\t2010\\t[EBo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0          Project\\tGutenberg's\\tArmageddon--2419\\tA\n",
       "1                                                  D\n",
       "2  ,\\tby\\tPhilip\\tFrancis\\tNowlan This\\teBook\\tis...\n",
       "3  \\t\\tYou\\tmay\\tcopy\\tit,\\tgive\\tit\\taway\\tor\\n\\...\n",
       "4                                          gutenberg\n",
       "5                 org\\n\\nTitle:\\tArmageddon--2419\\tA\n",
       "6                                                  D\n",
       "7  \\n\\nAuthor:\\tPhilip\\tFrancis\\tNowlan\\n\\nIllust...\n",
       "8  \\tPaul\\n\\nRelease\\tDate:\\tMay\\t26,\\t2010\\t[EBo...\n",
       "9                                                  D"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfrom the data to pandas dataframe\n",
    "text = pd.DataFrame(data.split('.'), columns=['text'])\n",
    "text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x.replace('\\t', ' ') for x in data]\n",
    "data = [x.replace('\\n', ' ') for x in data]\n",
    "data = [x.replace('\\r', ' ') for x in data]\n",
    "data = [x.replace('\"', '') for x in data]\n",
    "data = [x.replace(\"'\", '') for x in data]\n",
    "data = [x for x in data if x != ' ']\n",
    "data = [x for x in data if x != '']\n",
    "data = [x for x in data if len(x.split()) > 1]\n",
    "\n",
    "# delete more than 1 spaces\n",
    "data = [x.replace('  ', ' ') for x in data]\n",
    "data = [x.replace('   ', ' ') for x in data]\n",
    "data = [x.replace('    ', ' ') for x in data]\n",
    "data = [x.replace('     ', ' ') for x in data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e33c6f12404a0f9c960d29e2eb1820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assemble lines: concatenate title and description\n",
    "from nltk import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "lines = [\n",
    "    ' '.join(\n",
    "        tokenizer.tokenize(line.lower())\n",
    "    ) for line in tqdm(data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# special tokens: \n",
    "# - unk represents absent tokens, \n",
    "# - eos is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "\n",
    "    for line in tqdm(lines, desc='N-grams'):\n",
    "        unk_prefix = ' '.join([UNK] * (n - 1))\n",
    "        eos_suffix = EOS\n",
    "        tokens = f'{unk_prefix} {line} {eos_suffix}'.split()\n",
    "        for i in range(n - 1, len(tokens)):\n",
    "            n_gram = tuple(tokens[i - n + 1: i])\n",
    "            counts[n_gram].update([tokens[i]])\n",
    "\n",
    "    \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 73856.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_counts[('_UNK_', 'a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for k,v in tqdm(counts.items()):\n",
    "            s = sum(v.values())\n",
    "            for word, cout in v.items():\n",
    "                self.probs[k][word] = counts[k][word] / s \n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 103845.11it/s]\n",
      "100%|██████████| 321/321 [00:00<00:00, 546638.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1500/1500 [00:00<00:00, 13710.64it/s]\n",
      "100%|██████████| 19594/19594 [00:00<00:00, 665779.80it/s]\n"
     ]
    }
   ],
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    next_tokens = lm.get_possible_next_tokens(prefix)\n",
    "    if temperature == 0:\n",
    "        sorted_next_tokens = dict(\n",
    "            sorted(tuple(next_tokens.items()), key=lambda x:x[1], \n",
    "                   reverse=True)\n",
    "        )\n",
    "        next_token = tuple(sorted_next_tokens.items())[0][0]\n",
    "    else:\n",
    "        sum_probs = sum([\n",
    "            prob ** (1 / temperature) for prob in next_tokens.values()\n",
    "        ])\n",
    "\n",
    "        next_tokens = {\n",
    "            token: prob ** (1 / temperature) / sum_probs\n",
    "            for token, prob in next_tokens.items()\n",
    "        }\n",
    "        tokens = list(next_tokens.keys())\n",
    "        probs = list(next_tokens.values())\n",
    "        next_token = np.random.choice(tokens, 1, p=probs)[0]\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "test_freqs = Counter([get_next_token(lm, 'have') for _ in range(10000)])\n",
    "\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'have', temperature=1.0) for _ in range(20000)])\n",
    "test_freqs = Counter([get_next_token(lm, 'have', temperature=0.5) for _ in range(20000)])\n",
    "test_freqs = Counter([get_next_token(lm, 'have', temperature=0.0) for _ in range(20000)])\n",
    "\n",
    "print(\"Looks nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have a feeling something is going to mean real business for all we could _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'have' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interesting prophecies , of course , i think we can train all our gangs to use it under the arms , she would have been observed ? i asked _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'interesting prophecies' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10 ** -7.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    total_length = 0\n",
    "    log_pp = 0\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        tokens = [''] + line.split(' ') + [EOS]\n",
    "\n",
    "        for t in range(1, len(tokens)):\n",
    "            prefix = ' '.join(tokens[:t])\n",
    "            log_pp += max(\n",
    "                min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n",
    "            )\n",
    "            total_length += 1\n",
    "    \n",
    "    return np.exp(-( 1 / total_length) * log_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 194541.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 151528.32it/s]\n",
      "100%|██████████| 321/321 [00:00<00:00, 576111.08it/s]\n",
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 121222.66it/s]\n",
      "100%|██████████| 333/333 [00:00<00:00, 546868.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 62036.74it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 71832.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 76804.69it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/ipykernel_17793/1886438417.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n",
      "100%|██████████| 1/1 [00:00<00:00, 4809.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=86.300 ppx3=2.654 ppx10=2.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 60173.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1030.79it/s]\n",
      "  0%|          | 0/375 [00:00<?, ?it/s]/var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/ipykernel_17793/1886438417.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  min_logprob, np.log(lm.get_next_token_prob(prefix, tokens[t]))\n",
      "100%|██████████| 375/375 [00:00<00:00, 14918.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 851.52224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 49750.04it/s]\n",
      "100%|██████████| 4007/4007 [00:00<00:00, 494267.45it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 10976.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 14903.66023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 35270.75it/s]\n",
      "100%|██████████| 15712/15712 [00:00<00:00, 218872.26it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 8508.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 639725.17200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 45844.40it/s]\n",
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 103282.54it/s]\n",
      "N-grams: 100%|██████████| 100/100 [00:00<00:00, 138334.56it/s]\n"
     ]
    }
   ],
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 60250.68it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 9018.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 854.38045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 43893.47it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 11394.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 810.43758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N-grams: 100%|██████████| 1125/1125 [00:00<00:00, 33264.19it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 9701.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 2224.67414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ebccff7ae44889b433bf0e502ccf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sorted(lines, key=len)[:3]\n",
    "\n",
    "# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n",
    "from nltk import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "lines = [\n",
    "    ' '.join(\n",
    "        tokenizer.tokenize(line.lower())\n",
    "    ) for line in tqdm(lines)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(lines, test_size=0.2)\n",
    "lm_datasets = {'train' : train, 'valid' : valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "my_dict = {\"text\": lines}\n",
    "datasets = Dataset.from_dict(my_dict)\n",
    "tr_test_datasets = datasets.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_model_checkpoint = \"gpt2\"\n",
    "clm_tokenizer_checkpoint = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model, AutoModelForCausalLM\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('zaaabik/gpt2-arxiv-clm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9b27495c5147af815f73952e789020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e5db1088764d2ca0534358448a0cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = tr_test_datasets.map(tokenize_function, \n",
    "                                          batched=True, num_proc=4, \n",
    "                                          remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06ba31821d9457cb95f2720f43caa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a3c100db3a43fa8e3f1fb47c6ac79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20212,\n",
       " 837,\n",
       " 3737,\n",
       " 9930,\n",
       " 1392,\n",
       " 530,\n",
       " 1128,\n",
       " 26842,\n",
       " 8524,\n",
       " 319,\n",
       " 262,\n",
       " 954,\n",
       " 286,\n",
       " 1115,\n",
       " 837,\n",
       " 2686,\n",
       " 510,\n",
       " 883,\n",
       " 3172,\n",
       " 14127,\n",
       " 24823,\n",
       " 851,\n",
       " 477,\n",
       " 286,\n",
       " 606,\n",
       " 851,\n",
       " 290,\n",
       " 329,\n",
       " 11858,\n",
       " 11060,\n",
       " 837,\n",
       " 17666,\n",
       " 2051,\n",
       " 1169,\n",
       " 1700,\n",
       " 286,\n",
       " 262,\n",
       " 11087,\n",
       " 8286,\n",
       " 1570,\n",
       " 6816,\n",
       " 2523,\n",
       " 1866,\n",
       " 286,\n",
       " 262,\n",
       " 5462,\n",
       " 14281,\n",
       " 3812,\n",
       " 262,\n",
       " 8286,\n",
       " 1128,\n",
       " 532,\n",
       " 26842,\n",
       " 17301,\n",
       " 287,\n",
       " 281,\n",
       " 2230,\n",
       " 284,\n",
       " 4474,\n",
       " 10107,\n",
       " 1630,\n",
       " 286,\n",
       " 340,\n",
       " 837,\n",
       " 290,\n",
       " 2620,\n",
       " 262,\n",
       " 10303,\n",
       " 72,\n",
       " 1043,\n",
       " 340,\n",
       " 880,\n",
       " 510,\n",
       " 287,\n",
       " 262,\n",
       " 9686,\n",
       " 286,\n",
       " 262,\n",
       " 4074,\n",
       " 1462,\n",
       " 16565,\n",
       " 837,\n",
       " 3387,\n",
       " 3187,\n",
       " 1058,\n",
       " 2638,\n",
       " 1058,\n",
       " 1003,\n",
       " 279,\n",
       " 4743,\n",
       " 1878,\n",
       " 896,\n",
       " 1597,\n",
       " 2607,\n",
       " 318,\n",
       " 5140,\n",
       " 379,\n",
       " 807,\n",
       " 2931,\n",
       " 5093,\n",
       " 20007,\n",
       " 7421,\n",
       " 837,\n",
       " 8268,\n",
       " 13546,\n",
       " 1748,\n",
       " 837,\n",
       " 3384,\n",
       " 9508,\n",
       " 18298,\n",
       " 837,\n",
       " 357,\n",
       " 807,\n",
       " 486,\n",
       " 1267,\n",
       " 642,\n",
       " 4846,\n",
       " 532,\n",
       " 1248,\n",
       " 5774,\n",
       " 837,\n",
       " 3053,\n",
       " 1597,\n",
       " 2488,\n",
       " 279,\n",
       " 4743,\n",
       " 1878,\n",
       " 1659]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train'][1]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20212, 837, 3737, 9930, 1392, 530, 1128, 26842, 8524, 319]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train'][1]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"gpt2-arxiv-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    # push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets['train'],\n",
    "    eval_dataset=lm_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 250\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5348751d34424daae392312e0a94a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d674287d6f94d29bee4230cde40c3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.658186912536621, 'eval_runtime': 6.2495, 'eval_samples_per_second': 4.16, 'eval_steps_per_second': 0.64, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db77c017ebd4bd3ace826a5701456fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.51870059967041, 'eval_runtime': 6.5539, 'eval_samples_per_second': 3.967, 'eval_steps_per_second': 0.61, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5e74ab72f0436a8c2f3e7b0cc91bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.483121871948242, 'eval_runtime': 5.8213, 'eval_samples_per_second': 4.466, 'eval_steps_per_second': 0.687, 'epoch': 3.0}\n",
      "{'train_runtime': 685.8263, 'train_samples_per_second': 1.094, 'train_steps_per_second': 0.14, 'train_loss': 4.59431521097819, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=4.59431521097819, metrics={'train_runtime': 685.8263, 'train_samples_per_second': 1.094, 'train_steps_per_second': 0.14, 'train_loss': 4.59431521097819, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002fe53116fe4317947342ffd07a8a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 88.51\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /Users/markkhaus/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick_name = 'markkut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/tmp1hzocqaq/tokenizer_config.json\n",
      "Special tokens file saved in /var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/tmp1hzocqaq/special_tokens_map.json\n",
      "Uploading the following files to Markkut/gpt2-author-clm_3: tokenizer_config.json,special_tokens_map.json,merges.txt,vocab.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Markkut/gpt2-author-clm_3/commit/7fe280c249be1fad392e817251c364ddee3645fd', commit_message='Upload tokenizer', commit_description='', oid='7fe280c249be1fad392e817251c364ddee3645fd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\n",
    "    'gpt2-author-clm_3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/tmp4v2lvwfx/config.json\n",
      "Configuration saved in /var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/tmp4v2lvwfx/generation_config.json\n",
      "Model weights saved in /var/folders/rr/39767lj15dg81g2dbl4jq3rw0000gn/T/tmp4v2lvwfx/pytorch_model.bin\n",
      "Uploading the following files to Markkut/gpt2-author-clm_3: config.json,generation_config.json,pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501234cd325e464c9957cc3b1fa27b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db98774623345ec989d065f1871ab51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Markkut/gpt2-author-clm_3/commit/c90269448e836fc1e554eacf224ecd7a0cd7c8d8', commit_message='Upload model', commit_description='', oid='c90269448e836fc1e554eacf224ecd7a0cd7c8d8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"gpt2-author-clm_3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438c8ff4e205463abfd761afe8acb1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/925 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/markkhaus/.cache/huggingface/hub/models--markkut--gpt2-author-clm_3/snapshots/c90269448e836fc1e554eacf224ecd7a0cd7c8d8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"markkut/gpt2-author-clm_3\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/markkhaus/.cache/huggingface/hub/models--markkut--gpt2-author-clm_3/snapshots/c90269448e836fc1e554eacf224ecd7a0cd7c8d8/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"markkut/gpt2-author-clm_3\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc6b5dc5d4e41cc9e6d7bdbd7e4cfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /Users/markkhaus/.cache/huggingface/hub/models--markkut--gpt2-author-clm_3/snapshots/c90269448e836fc1e554eacf224ecd7a0cd7c8d8/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at markkut/gpt2-author-clm_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbdb1fae52e4ded83c867dbec0ab2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /Users/markkhaus/.cache/huggingface/hub/models--markkut--gpt2-author-clm_3/snapshots/c90269448e836fc1e554eacf224ecd7a0cd7c8d8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model = f'{nick_name}/gpt2-author-clm_3',\n",
    "    tokenizer = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "/opt/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Armrageddon is a game about getting away faster with all your might and in anticipation that another titan would pass him through the city while he was still far away, or else there would be something like a pikeman in my front, or perhaps'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator('Armrageddon is a game about')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'hello kitty ik : so that i dont hear it the best i wish i had, i thought, that i might as well not come this way, i thought, in so doing i would die on the spot, and so i could live'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator('hello kitty ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
